{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract MTA Data from archives\n",
    "\n",
    "This notebook helps automate extracting data from the protobufs manually downloaded from [Historical GTFS data](http://web.mta.info/developers/data/archives.html)the latest source suggested at:\n",
    "https://groups.google.com/d/msg/mtadeveloperresources/Whm5XTVINcE/z-LO12ANAAAJ\n",
    "\n",
    "Note that another S3 hosted [historical datasource](http://web.mta.info/developers/MTA-Subway-Time-historical-data.html) referenced on the MTA website, but this is outdated, and the above MTA Alert Archive is correct.\n",
    "\n",
    "NOTE: This notebook assumes that the protobufs have already been downloaded to <code>data/raw/status</code> e.g. <code>data/raw/status/201901.zip</code> from http://web.mta.info/developers/data/archives.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "data_dir = '../data/raw/status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../data/raw/status/201808.zip', '../data/raw/status/201809.zip', '../data/raw/status/201810.zip', '../data/raw/status/201811.zip', '../data/raw/status/201812.zip', '../data/raw/status/201901.zip', '../data/raw/status/201902.zip', '../data/raw/status/201903.zip', '../data/raw/status/201905.zip', '../data/raw/status/201904.zip', '../data/raw/status/201906.zip']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "protobuf_paths = glob.glob('{}/[0-9]*.zip'.format(data_dir))\n",
    "\n",
    "if len(protobuf_paths) == 0:\n",
    "    raise ValueError('No matching protbufs found in {}, please download from https://m.mymtaalerts.com/archive')\n",
    "    \n",
    "print(protobuf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper cell for recursively unzipping monthly rollups\n",
    "This is a bit finnicky, as the layout and zip format vary from month-to month, but this is a helpful tool for unzipping some of the.  This will fail for a handful of the monthly archives, and you will either need to modify it, or manually handle those cases.  Especially watch out for <code>201812.zip</code>, as that contains <code>201812.7z</code>\n",
    "\n",
    "Additionally, there are a small number of corrupted daily zips, so this absorbs and logs those errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: ../data/raw/status/201906.zip\n",
      "1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|##                                                       |failures: ------"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-31d65ac76a8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mfailures\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m                 \u001b[0;31m# and give a timeout to avoid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                     \u001b[0;31m# write directly to __stderr__ instead of warning because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m                     \u001b[0;31m# if this is happening sys.stderr may be the problem.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/miniconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import shutil\n",
    "import progressbar\n",
    "import io\n",
    "\n",
    "# Keep a list of files with failed extractions\n",
    "failed_files = os.path.join(data_dir, 'failures.txt')\n",
    "\n",
    "force = False\n",
    "\n",
    "# unzip monthly rollups, then unzip the daily files inside\n",
    "# This code is largely copied from: https://stackoverflow.com/questions/36285502/how-to-extract-zip-file-recursively-in-python\n",
    "# The daily zipfiles are ~1GB, so there are big speed gains from unzipping in memory\n",
    "#for monthly_file in protobuf_paths[-1:]:\n",
    "for monthly_file in ['../data/raw/status/201906.zip',]:\n",
    "    widgets = [progressbar.Percentage(), progressbar.Bar(), progressbar.Variable('failures')]    \n",
    "\n",
    "    \n",
    "    print(\"Extracting: \" + monthly_file)\n",
    "    z = zipfile.ZipFile(monthly_file)\n",
    "    for i,f in enumerate(z.namelist()):\n",
    "        print(\"{}/{}\".format(i+1, len(z.namelist())))\n",
    "        # get directory name from file\n",
    "        dirname = os.path.join(data_dir, os.path.splitext(f)[0])\n",
    "        # create new directory\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "        # read inner zip file into bytes buffer \n",
    "        content = io.BytesIO(z.read(f))\n",
    "        zip_file = zipfile.ZipFile(content)\n",
    "        \n",
    "        # Skip if already unzipped\n",
    "        if not force:\n",
    "            if len(glob.glob(dirname+'/*')) == len(zip_file.namelist()):\n",
    "                print(\"Skipping \" + os.path.basename(dirname))\n",
    "                continue\n",
    "         \n",
    "        # Iterate through in-memory zipfile, dumping sub-minutely protobufs into daily directories\n",
    "        bar = progressbar.ProgressBar(widgets=widgets, max_value=len(zip_file.namelist()), min_poll_interval=.5).start()\n",
    "        failures = 0\n",
    "        for j,f2 in enumerate(zip_file.namelist()):\n",
    "            try:\n",
    "                zip_file.extract(f2, dirname)\n",
    "            except Exception as e:\n",
    "                # At the moment, some messages a sporadically unable to parse\n",
    "                with io.open(failed_files, 'a') as fh:\n",
    "                    fh.write(f2+'\\n')\n",
    "                failures += 1\n",
    "                \n",
    "            sys.stdout.flush()\n",
    "            bar.update(j+1, failures=failures)\n",
    "        zip_file.close()\n",
    "        \n",
    "        bar.finish()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract GTFS into H5\n",
    "Extract relevant info from GTFS files to H5.  Right now this is just extracting the current train in a station at a given time, and information about which train is expected to arrive next, and when.  It is possible that more information will be added, but this is too heavy-weight to include in [DatExploration.ipynb](DataExploration.ipyny), and should only be run as part of data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'MTADelayPredict.utils.gtfs_loader' from '/opt/project/MTADelayPredict/utils/gtfs_loader.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relative MTADelayPredict Project\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.realpath(os.path.join('DataExploration.ipynb')))))\n",
    "from MTADelayPredict.utils import gtfs_loader\n",
    "from importlib import reload\n",
    "reload(gtfs_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loaders for loading data in monthly chunks\n",
    "loader_start = pd.Timestamp('2018-08-01 00:00:00')\n",
    "loader_end =  pd.Timestamp('2019-07-01 00:00:00')\n",
    "drange = pd.date_range(start=loader_start, end=loader_end, freq='M')\n",
    "loaders = []\n",
    "\n",
    "# Build separate loaders per month\n",
    "for i,m in enumerate(drange):\n",
    "    start_date = m.replace(day=1)\n",
    "    end_date = m.replace(hour=23,minute=59,second=59)\n",
    "    loader = gtfs_loader.GTFSLoader(data_dir=os.path.join('../data/raw/status'), \\\n",
    "                                train_line='nqrw')\n",
    "    loaders.append((loader, start_date, end_date))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|                                     |entries:   5096decode_errors:     35"
     ]
    }
   ],
   "source": [
    "# Load months in a pool and cache to h5\n",
    "from multiprocessing import Pool\n",
    "#STOP_FILTER = '^R16N$'\n",
    "STOP_FILTER = '^.*N$'\n",
    "ROUTE_FILTER = 'N'\n",
    "\n",
    "def f(args):\n",
    "    loader, start_date, end_date = args\n",
    "    loader.load_range(start_date, end_date, stop_filter=STOP_FILTER, route_filter=ROUTE_FILTER, verbose=True)\n",
    "    filename = os.path.join(data_dir, 'status_{:04d}{:02d}.h5'.format(start_date.year, start_date.month))\n",
    "    loader.stopped_at_df.to_hdf(filename, key='stopped_at')\n",
    "    loader.next_train_df.to_hdf(filename, key='next_train')\n",
    "    loader.next_scheduled_arrival_df.to_hdf(filename, key='next_scheduled_arrival')\n",
    "    \n",
    "    return \"{} - {} Done\".format(start_date, end_date)\n",
    "\n",
    "with Pool(6) as p:\n",
    "     print(p.map(f, loaders))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
